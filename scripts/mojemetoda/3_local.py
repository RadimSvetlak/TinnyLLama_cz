#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
fast_dataset_generator.py
- Optimalizov√°no pro slab≈°√≠ PC
- Anglick√© instrukce pro model, ƒçesk√© prompty do tr√©novac√≠ch dat
- Pr≈Øbƒõ≈æn√© ukl√°d√°n√≠, minim√°ln√≠ overhead
"""

import os, sys, json, time, random, re
from pathlib import Path
from typing import List, Optional, Dict

try:
    from llama_cpp import Llama
except Exception as e:
    print("‚ùå Chyb√≠ llama-cpp-python", file=sys.stderr)
    sys.exit(1)

# ======= KONFIGURACE PRO SLAB≈†√ç PC =======
MODEL_PATH = r"D:\AI\gpt-oss\gemma-3n-E4B-it-Q8_0.gguf"
MODEL_PATH = r"D:\AI\gemma3\google_gemma-3-4b-it-qat-Q6_K_L.gguf"
INPUT_FILE = "moje_metoda.txt"
OUTPUT_FILE = None

# OPTIMALIZACE PRO SLAB≈†√ç HW
CHUNK_SIZE = 50           # mal√© chunky pro rychlost
OVERLAP = 10               # minim√°ln√≠ overlap
TASKS_PER_CHUNK = 6       # v√≠ce √∫kol≈Ø na chunk
MAX_TOKENS = 80           # kr√°tk√© odpovƒõdi
TEMPERATURE = 0.5         # ni≈æ≈°√≠ pro stabilitu
BATCH_SIZE = 128           # mal√Ω batch pro slab≈°√≠ PC
N_CTX = 1024              # men≈°√≠ kontext
DELAY = 0.0               # ≈æ√°dn√Ω delay

# ======= MAPOV√ÅN√ç √öKOL≈Æ =======
# Instrukce pro model (anglicky) ‚Üí ƒåesk√© prompty pro tr√©novac√≠ data
TASK_MAP = {
    'paraphrase': {
        'model_instruction': 'Rewrite the following text using different words but keep the same meaning. Answer only in Czech, without English translations unless explicitly asked:',
        'training_prompt': 'Parafr√°zuj tento text:'
    },
    'summarize': {
        'model_instruction': 'Summarize this text in 1-2 sentences. Answer only in Czech, without English translations unless explicitly asked:',
        'training_prompt': 'Shr≈à tento text:'
    },
    'expand': {
        'model_instruction': 'Expand this text into 2-3 sentences with more details. Answer only in Czech, without English translations unless explicitly asked:',
        'training_prompt': 'Rozveƒè tento text:'
    },
    'simplify': {
        'model_instruction': 'Explain this text in simple words for children. Answer only in Czech, without English translations unless explicitly asked:',
        'training_prompt': 'Vysvƒõtli jednodu≈°e:'
    },
    'keywords': {
        'model_instruction': 'Extract 3-5 key words from this text. Answer only in Czech, without English translations unless explicitly asked:',
        'training_prompt': 'Vypi≈° kl√≠ƒçov√° slova z textu:'
    },
    'title': {
        'model_instruction': 'Create a short title for this text (max 6 words). Answer only in Czech, without English translations unless explicitly asked:',
        'training_prompt': 'Vytvo≈ô titulek pro text:'
    },
    'question': {
        'model_instruction': 'Create a question that can be answered from this text. Answer only in Czech, without English translations unless explicitly asked:',
        'training_prompt': 'Vytvo≈ô ot√°zku k textu:'
    },
    'continue': {
        'model_instruction': 'Write the next logical sentence that would follow this text. Answer only in Czech, without English translations unless explicitly asked:',
        'training_prompt': 'Pokraƒçuj v textu dal≈°√≠ vƒõtou:'
    }
}

TASK_LIST = list(TASK_MAP.keys())

def read_file(path: Path) -> str:
    """Rychl√© ƒçten√≠ souboru."""
    try:
        return path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        return path.read_text(encoding="cp1250")

def make_chunks(text: str) -> List[str]:
    """Rychl√© rozdƒõlen√≠ na chunky."""
    # Jednoduch√° tokenizace
    words = re.sub(r'\s+', ' ', text.replace('\n', ' ')).split()
    
    chunks = []
    step = CHUNK_SIZE - OVERLAP
    
    for i in range(0, len(words), step):
        chunk = words[i:i + CHUNK_SIZE]
        if len(chunk) >= 15:  # min d√©lka
            chunks.append(' '.join(chunk))
        if i + CHUNK_SIZE >= len(words):
            break
    
    return chunks

def clean_response(text: str) -> str:
    text = text.strip()
    # p≈ô√≠padnƒõ jen lehk√© oƒçi≈°tƒõn√≠ bez o≈ôezu:
    text = re.sub(r'<[^>]*>', '', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text

class FastDataGen:
    def __init__(self):
        self.llm = None
        self.total_items = 0
        self.start_time = None
    
    def init_model(self):
        """Rychl√© naƒçten√≠ modelu."""
        print("üîÑ Naƒç√≠t√°m model...")
        start = time.time()
        
        self.llm = Llama(
            model_path=MODEL_PATH,
            n_ctx=N_CTX,
            n_threads=min(4, os.cpu_count() or 2),  # max 4 vl√°kna
            n_gpu_layers=-1,  # -1 = auto
            verbose=False,
            n_batch=BATCH_SIZE,
            f16_kv=True,
            use_mlock=False,
        )
        
        # Warm-up
        self.llm("Hi", max_tokens=1, temperature=0)
        print(f"‚úÖ Ready ({time.time() - start:.1f}s)")
    
    def generate(self, task: str, chunk: str) -> Optional[Dict[str, str]]:
        """Jedna rychl√° generace."""
        task_info = TASK_MAP[task]
        
        # Prompt pro model (anglicky)
        model_prompt = f"{task_info['model_instruction']}\n\n{chunk}\n\nAnswer:"
        # print("PROMT = " + str(model_prompt) + "\n")
        try:
            response = self.llm(
                model_prompt,
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE,
                top_p=0.9,
                echo=False,
            )
            
            completion = clean_response(response["choices"][0]["text"])
            
            if len(completion.strip()) < 5:
                return None
            
            # Tr√©novac√≠ prompt (ƒçesky)
            training_prompt = f"{task_info['training_prompt']} {chunk}"
            
            return {
                'prompt': training_prompt,
                'completion': completion
            }
            
        except Exception:
            return None
    
    def save_item(self, item: Dict[str, str], file_path: Path):
        """Ulo≈æ√≠ pouze prompt a completion (bez context)."""
        clean_item = {
            'prompt': item['prompt'],
            'completion': item['completion']
        }
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(clean_item, ensure_ascii=False) + "\n")
    
    def process(self, input_file: Path, output_file: Path):
        """Hlavn√≠ zpracov√°n√≠."""
        text = read_file(input_file)
        chunks = make_chunks(text)
        
        if not chunks:
            print("‚ùå ≈Ω√°dn√© chunky")
            return
        
        # P≈ôiprav v√Ωstup
        output_file.parent.mkdir(exist_ok=True)
        if output_file.exists():
            output_file.unlink()
        
        print(f"üìÑ {len(chunks)} chunk≈Ø")
        print(f"üíæ {output_file.name}")
        print(f"‚ö° {TASKS_PER_CHUNK} √∫kol≈Ø/chunk\n")
        
        self.total_items = 0
        self.start_time = time.time()
        
        try:
            for i, chunk in enumerate(chunks, 1):
                # Random √∫koly pro tento chunk
                tasks = random.sample(TASK_LIST, min(TASKS_PER_CHUNK, len(TASK_LIST)))
                chunk_count = 0
                
                for task in tasks:
                    item = self.generate(task, chunk)
                    if item:
                        self.save_item(item, output_file)
                        chunk_count += 1
                        self.total_items += 1
                    
                    if DELAY > 0:
                        time.sleep(DELAY)
                
                # Progress ka≈æd√Ωch 10 chunk≈Ø
                if i % 10 == 0 or i == len(chunks):
                    elapsed = time.time() - self.start_time
                    rate = self.total_items / elapsed if elapsed > 0 else 0
                    progress = (i / len(chunks)) * 100
                    
                    print(f"{i:3d}/{len(chunks)} ({progress:4.1f}%) | "
                          f"{self.total_items:4d} items | "
                          f"{rate:5.1f}/s")
            
            # Fin√°ln√≠ stats
            elapsed = time.time() - self.start_time
            print(f"\nüéâ {self.total_items} polo≈æek za {elapsed:.1f}s")
            print(f"‚ö° {self.total_items/elapsed:.1f} polo≈æek/s")
            
        except KeyboardInterrupt:
            print(f"\n‚ö†Ô∏è P≈ôeru≈°eno - ulo≈æeno {self.total_items} polo≈æek")

def main():
    input_path = Path(INPUT_FILE)
    if not input_path.exists():
        print(f"‚ùå {input_path} neexistuje")
        sys.exit(1)
    
    output_path = Path(OUTPUT_FILE or f"{input_path.stem}_dataset.jsonl")
    
    print("üöÄ RYCHL√ù GENER√ÅTOR DATASETU")
    print(f"üéØ {input_path.name} ‚Üí {output_path.name}")
    print(f"üíª Optimalizov√°no pro slab≈°√≠ PC\n")
    
    generator = FastDataGen()
    
    try:
        generator.init_model()
        generator.process(input_path, output_path)
    except Exception as e:
        print(f"‚ùå {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()