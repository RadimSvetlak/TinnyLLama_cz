# ------------------------------------------------------------
# NÃ¡zev: GenerÃ¡tor trÃ©ninkovÃ½ch otÃ¡zek k vÄ›tÃ¡m s kontextem shrnutÃ­ odstavce
# Popis:
#   Tento skript zpracovÃ¡vÃ¡ textovÃ© soubory (.txt) a pro kaÅ¾dÃ½ odstavec:
#     1) Vygeneruje struÄnÃ© shrnutÃ­ odstavce pomocÃ­ lokÃ¡lnÃ­ho jazykovÃ©ho modelu.
#     2) RozdÄ›lÃ­ odstavec na jednotlivÃ© vÄ›ty.
#     3) Pro kaÅ¾dou vÄ›tu vygeneruje s vyuÅ¾itÃ­m modelu sadu jednoduchÃ½ch otÃ¡zek 
#        (kdo, co, kdy, kde, proÄ, jak, s kÃ½m, ÄÃ­m, kolik) s ohledem na shrnutÃ­
#        odstavce, aby mÄ›ly lepÅ¡Ã­ kontext.
#   VÃ½sledkem jsou pÃ¡ry promptâ€“completion, kde prompt je otÃ¡zka a completion
#   je pÅ¯vodnÃ­ vÄ›ta, vhodnÃ© pro trÃ©nink menÅ¡Ã­ch jazykovÃ½ch modelÅ¯.
#
# Vstupy:
#   - TextovÃ© soubory ve sloÅ¾ce INPUT_DIR.
#   - Parametry pro model (MODEL_PATH, N_QUESTIONS, MAX_TOKENS).
#
# VÃ½stupy:
#   - Soubor OUTPUT_PATH s Å™Ã¡dky ve formÃ¡tu:
#       prompt: <otÃ¡zka>, completion: <pÅ¯vodnÃ­ vÄ›ta>
#
# KlÃ­ÄovÃ© funkce:
#   - safe_read_text: NaÄÃ­tÃ¡nÃ­ textu s fallback kÃ³dovÃ¡nÃ­m.
#   - split_into_paragraphs: RozdÄ›lenÃ­ textu na odstavce.
#   - split_into_sentences: RozdÄ›lenÃ­ odstavce na jednotlivÃ© vÄ›ty.
#   - make_summary_prompt: VytvoÅ™enÃ­ promptu pro shrnutÃ­ odstavce.
#   - make_sentence_prompt_with_context: VytvoÅ™enÃ­ promptu pro otÃ¡zky 
#        k vÄ›tÄ› s kontextem shrnutÃ­.
#   - summarize_paragraph: VolÃ¡nÃ­ modelu pro zÃ­skÃ¡nÃ­ shrnutÃ­ odstavce.
#   - try_parse_questions: BezpeÄnÃ© parsovÃ¡nÃ­ JSON vÃ½stupu otÃ¡zek z modelu.
#   - sanitize_for_line: OÄiÅ¡tÄ›nÃ­ Å™etÄ›zce pro zÃ¡pis na jeden Å™Ã¡dek.
#
# ZÃ¡vislosti:
#   - Python 3.8+
#   - llama-cpp-python
#   - StandardnÃ­ knihovny: os, re, json
#
# ------------------------------------------------------------


import os
import re
from typing import List, Optional
from llama_cpp import Llama
import json

# ====== NastavenÃ­ ======
MODEL_PATH = r"D:\AI\gemma_test\google_gemma-3-4b-it-qat-Q8_0.gguf"
INPUT_DIR = "./"
OUTPUT_PATH = "train_prompts.txt"
N_QUESTIONS = 3                   # pÅ™esnÄ› 3 otÃ¡zky na vÄ›tu
MAX_TOKENS = 512                  # krÃ¡tkÃ© vÃ½stupy (shrnutÃ­ / otÃ¡zky)
MIN_SENT_LEN = 12                 # minimÃ¡lnÃ­ dÃ©lka vÄ›ty (znaky), jinak skip
VERBOSE = True

# ====== Inicializace modelu ======
llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=8192,
    n_threads=8,
    n_gpu_layers=-1,
    verbose=False,
    temperature=0.7,
    top_k=50,
    top_p=0.9,
    repeat_penalty=1.05,
)

# ====== NaÄÃ­tÃ¡nÃ­ s fallback kÃ³dovÃ¡nÃ­m ======
def safe_read_text(path: str) -> str:
    for enc in ("utf-8", "windows-1250", "iso-8859-2"):
        try:
            with open(path, "r", encoding=enc) as f:
                return f.read()
        except UnicodeDecodeError:
            continue
    print(f"âš  Nelze naÄÃ­st soubor kvÅ¯li kÃ³dovÃ¡nÃ­: {path}")
    return ""

# ====== DÄ›lenÃ­ na odstavce ======
def split_into_paragraphs(text: str) -> List[str]:
    # PrimÃ¡rnÄ› dÄ›l na prÃ¡zdnÃ© Å™Ã¡dky; odstraÅˆ extrÃ©mnÄ› krÃ¡tkÃ©/whitespace bloky
    parts = re.split(r"(?:\r?\n){2,}", text.strip())
    paras = ["\n".join(p.strip().splitlines()) for p in parts if p and p.strip()]
    # Pokud nenÃ­ Å¾Ã¡dnÃ½ nebo jen jeden odstavec, nechej text tak jak je
    return paras if paras else [text.strip()]

# ====== HeuristickÃ© dÄ›lenÃ­ na vÄ›ty (bez NLTK/SciPy) ======
ABBREV = {
    "napÅ™", "atd", "apod", "tj", "tzn", "tzv", "mj", "aj", "ap",
    "ing", "mgr", "phdr", "phd", "bc", "bcaa", "bca", "bcs", "dr",
    "p", "str", "sv", "Ä", "ul", "tÅ™", "nÃ¡m", "r", "Äj", "ÄÃ­s", "pÅ™",
}

def split_into_sentences(text: str) -> List[str]:
    t = re.sub(r"[ \t]+", " ", text.replace("\r", " "))
    t = re.sub(r"\n+", " ", t).strip()

    sentences = []
    buf = []
    i = 0
    L = len(t)

    while i < L:
        ch = t[i]
        buf.append(ch)

        if ch in ".!?":
            prev_chunk = "".join(buf).rstrip()
            m = re.search(r"([\wÃÄŒÄÃ‰ÄšÃÅ‡Ã“Å˜Å Å¤ÃšÅ®ÃÅ½Ã¡ÄÄÃ©Ä›Ã­ÅˆÃ³Å™Å¡Å¥ÃºÅ¯Ã½Å¾]+)\.$", prev_chunk)
            prev_word = m.group(1).lower() if m else ""

            j = i + 1
            while j < L and t[j] in ['"', "â€", "â€œ", "Â»", "Â«", "â€™", "'", " "]:
                j += 1

            is_abbrev = prev_word in ABBREV
            next_cap = j < L and (t[j].isupper() or t[j].isdigit())

            if not is_abbrev and next_cap:
                sentence = "".join(buf).strip()
                if len(sentence) >= MIN_SENT_LEN:
                    sentences.append(sentence)
                buf = []
        i += 1

    tail = "".join(buf).strip()
    if len(tail) >= MIN_SENT_LEN:
        sentences.append(tail)

    sentences = [" ".join(s.split()) for s in sentences]
    return sentences

# ====== Prompty ======
def make_summary_prompt(paragraph: str) -> str:
    compact = " ".join(paragraph.split())
    return (
        "<bos><start_of_turn>user\n"
        "ShrÅˆ nÃ¡sledujÃ­cÃ­ odstavec do 1â€“2 velmi krÃ¡tkÃ½ch vÄ›t v ÄeÅ¡tinÄ›.\n"
        "- PouÅ¾ij jen klÃ­ÄovÃ© udÃ¡losti a subjekty.\n"
        "- Bez odrÃ¡Å¾ek, bez formÃ¡tovÃ¡nÃ­, bez zÃ¡vorek.\n"
        "<end_of_turn>\n"
        "<start_of_turn>user\n"
        f"Odstavec:\n{compact}\n"
        "<end_of_turn>\n"
        "<start_of_turn>model\n"
    )

def make_sentence_prompt_with_context(sentence: str, summary: str, n_questions: int = 3) -> str:
    return (
        "<bos><start_of_turn>user\n"
        "Vygeneruj jednoduchÃ© otÃ¡zky v ÄeÅ¡tinÄ› k jednÃ© vÄ›tÄ›, s ohledem na shrnutÃ­ odstavce.\n"
        "CÃ­l: pomoci malÃ©mu modelu uÄit se Äeskou vÄ›tnou stavbu.\n"
        f"- VytvoÅ™ pÅ™esnÄ› {n_questions} krÃ¡tkÃ½ch otÃ¡zek (A1â€“A2) typu kdo, co, kdy, kde, proÄ, jak, s kÃ½m, ÄÃ­m, kolik.\n"
        "- NepouÅ¾Ã­vej externÃ­ znalosti, vyhni se otÃ¡zkÃ¡m ano/ne.\n"
        "VÃ½stup: JSON pole objektÅ¯ {{\"question\":\"...\"}} bez dalÅ¡Ã­ch textÅ¯.\n"
        "<end_of_turn>\n"
        "<start_of_turn>user\n"
        f"ShrnutÃ­ odstavce:\n{summary}\n\n"
        f"VÄ›ta:\n{sentence}\n"
        "<end_of_turn>\n"
        "<start_of_turn>model\n"
    )

# ====== ParsovÃ¡nÃ­ JSON vÃ½stupu (jen otÃ¡zky) ======
def try_parse_questions(s: str) -> Optional[list]:
    s = s.strip()
    start = s.find("[")
    end = s.rfind("]")
    if start != -1 and end != -1 and end > start:
        try:
            data = json.loads(s[start:end+1])
            if isinstance(data, list):
                qs = []
                for i in data:
                    if isinstance(i, dict) and "question" in i:
                        q = str(i["question"]).strip()
                        if q:
                            qs.append(q)
                return qs if qs else None
        except Exception:
            return None
    return None

# ====== Util: sanitizace pro formÃ¡t Å™Ã¡dku (zachovej diakritiku, odstraÅˆ ÄÃ¡rky) ======
def sanitize_for_line(s: str) -> str:
    s = s.replace(",", " ")
    s = " ".join(s.split())
    return s

def summarize_paragraph(paragraph: str) -> Optional[str]:
    prompt = make_summary_prompt(paragraph)
    try:
        resp = llm(prompt, max_tokens=128)
        summary = resp["choices"][0]["text"].strip()
        # lehkÃ¡ oÄista
        summary = " ".join(summary.split())
        return summary if summary else None
    except Exception as e:
        print(f"âš  Chyba pÅ™i shrnutÃ­ odstavce: {e}")
        return None

# ====== HlavnÃ­ bÄ›h ======
def main():
    files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(".txt")]
    total_files = len(files)
    total_pairs = 0

    with open(OUTPUT_PATH, "a", encoding="utf-8") as out_f:
        for file_index, filename in enumerate(files, start=1):
            if VERBOSE:
                print(f"ğŸ“‚ soubor {file_index}/{total_files}: {filename}")
            try:
                raw_text = safe_read_text(os.path.join(INPUT_DIR, filename))
            except Exception as e:
                print(f"âš  Nelze ÄÃ­st {filename}: {e}")
                continue

            if not raw_text.strip():
                continue

            paragraphs = split_into_paragraphs(raw_text)
            if VERBOSE:
                print(f"   âœ nalezeno odstavcÅ¯: {len(paragraphs)}")

            for pi, para in enumerate(paragraphs, start=1):
                if len(para.strip()) < MIN_SENT_LEN:
                    continue

                # 1) ShrnutÃ­ odstavce
                summary = summarize_paragraph(para)
                if not summary:
                    # Fallback: vezmi prvnÃ­ch ~200 znakÅ¯ odstavce jako "pseudo-shrnuti"
                    summary = " ".join(para.split())[:200]

                # 2) RozdÄ›lit odstavec na vÄ›ty
                sentences = split_into_sentences(para)
                if VERBOSE:
                    print(f"      - odstavec {pi}: vÄ›ty = {len(sentences)}")

                # 3) Pro kaÅ¾dou vÄ›tu generovat otÃ¡zky s kontextem shrnutÃ­
                for si, sentence in enumerate(sentences, start=1):
                    if len(sentence) < MIN_SENT_LEN:
                        continue

                    prompt = make_sentence_prompt_with_context(sentence, summary, N_QUESTIONS)
                    try:
                        resp = llm(prompt, max_tokens=MAX_TOKENS)
                        raw = resp["choices"][0]["text"]
                    except Exception as e:
                        print(f"âš  Chyba volÃ¡nÃ­ modelu u vÄ›ty {si} (odstavec {pi}): {e}")
                        continue

                    questions = try_parse_questions(raw)
                    if not questions:
                        # Fallback: zkuste snÃ­Å¾it teplotu
                        try:
                            resp2 = llm(prompt, max_tokens=MAX_TOKENS, temperature=0.3)
                            raw2 = resp2["choices"][0]["text"]
                            questions = try_parse_questions(raw2)
                        except Exception:
                            questions = None

                    if not questions:
                        continue

                    sent_clean = sanitize_for_line(sentence)
                    for q in questions:
                        q_clean = sanitize_for_line(q)
                        out_f.write(f"prompt: {q_clean}, completion: {sent_clean}\n")
                        out_f.flush()
                        total_pairs += 1

    print(f"âœ” Hotovo, uloÅ¾eno {total_pairs} pÃ¡rÅ¯ do {OUTPUT_PATH}")

if __name__ == "__main__":
    main()